{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "from PIL import Image, ImageFile \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import cv2 \n",
    "\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches \n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate Intersection over Union (IoU) \n",
    "def iou(box1, box2, is_pred=True): \n",
    "\tif is_pred: \n",
    "\t\t# IoU score for prediction and label \n",
    "\t\t# box1 (prediction) and box2 (label) are both in [x, y, width, height] format \n",
    "\t\t\n",
    "\t\t# Box coordinates of prediction \n",
    "\t\tb1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "\t\tb1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "\t\tb1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "\t\tb1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "\n",
    "\t\t# Box coordinates of ground truth \n",
    "\t\tb2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "\t\tb2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "\t\tb2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "\t\tb2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "\n",
    "\t\t# Get the coordinates of the intersection rectangle \n",
    "\t\tx1 = torch.max(b1_x1, b2_x1) \n",
    "\t\ty1 = torch.max(b1_y1, b2_y1) \n",
    "\t\tx2 = torch.min(b1_x2, b2_x2) \n",
    "\t\ty2 = torch.min(b1_y2, b2_y2) \n",
    "\t\t# Make sure the intersection is at least 0 \n",
    "\t\tintersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0) \n",
    "\n",
    "\t\t# Calculate the union area \n",
    "\t\tbox1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1)) \n",
    "\t\tbox2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1)) \n",
    "\t\tunion = box1_area + box2_area - intersection \n",
    "\n",
    "\t\t# Calculate the IoU score \n",
    "\t\tepsilon = 1e-6\n",
    "\t\tiou_score = intersection / (union + epsilon) \n",
    "\n",
    "\t\t# Return IoU score \n",
    "\t\treturn iou_score \n",
    "\t\n",
    "\telse: \n",
    "\t\t# IoU score based on width and height of bounding boxes \n",
    "\t\t\n",
    "\t\t# Calculate intersection area \n",
    "\t\tintersection_area = torch.min(box1[..., 0], box2[..., 0]) * torch.min(box1[..., 1], box2[..., 1]) \n",
    "\n",
    "\t\t# Calculate union area \n",
    "\t\tbox1_area = box1[..., 0] * box1[..., 1] \n",
    "\t\tbox2_area = box2[..., 0] * box2[..., 1] \n",
    "\t\tunion_area = box1_area + box2_area - intersection_area \n",
    "\n",
    "\t\t# Calculate IoU score \n",
    "\t\tiou_score = intersection_area / union_area \n",
    "\n",
    "\t\t# Return IoU score \n",
    "\t\treturn iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-maximum suppression function to remove overlapping bounding boxes \n",
    "def nms(bboxes, iou_threshold, threshold): \n",
    "\t# Filter out bounding boxes with confidence below the threshold. \n",
    "\tbboxes = [box for box in bboxes if box[0] > threshold]\n",
    "\n",
    "\t# Sort the bounding boxes by confidence in descending order. \n",
    "\tbboxes = sorted(bboxes, key=lambda x: x[0], reverse=True) \n",
    "\n",
    "\t# Initialize the list of bounding boxes after non-maximum suppression. \n",
    "\tbboxes_nms = [] \n",
    "\n",
    "\twhile bboxes: \n",
    "\t\t# Get the first bounding box. \n",
    "\t\tfirst_box = bboxes.pop(0) \n",
    "\n",
    "\t\t# Iterate over the remaining bounding boxes. \n",
    "\t\tfor box in bboxes: \n",
    "\t\t# If the bounding boxes do not overlap or if the first bounding box has \n",
    "\t\t# a higher confidence, then add the second bounding box to the list of \n",
    "\t\t# bounding boxes after non-maximum suppression. \n",
    "\t\t\tprint(box, first_box)\n",
    "\t\t\tif box[0] != first_box[0] or iou( \n",
    "\t\t\t\ttorch.tensor(first_box[2:]), \n",
    "\t\t\t\ttorch.tensor(box[2:]), \n",
    "\t\t\t) < iou_threshold: \n",
    "\t\t\t\t# Check if box is not in bboxes_nms \n",
    "\t\t\t\tif box not in bboxes_nms: \n",
    "\t\t\t\t\t# Add box to bboxes_nms \n",
    "\t\t\t\t\tbboxes_nms.append(box) \n",
    "\n",
    "\t# Return bounding boxes after non-maximum suppression. \n",
    "\treturn bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert cells to bounding boxes \n",
    "def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True): \n",
    "\t# Batch size used on predictions \n",
    "\tbatch_size = predictions.shape[0] \n",
    "\t# Number of anchors \n",
    "\tnum_anchors = len(anchors) \n",
    "\t# List of all the predictions \n",
    "\tbox_predictions = predictions[..., 1:] \n",
    "\n",
    "\t# If the input is predictions then we will pass the x and y coordinate \n",
    "\t# through sigmoid function and width and height to exponent function and \n",
    "\t# calculate the score and best class. \n",
    "\tif is_predictions: \n",
    "\t\tanchors = anchors.reshape(1, len(anchors), 1, 1, 2) \n",
    "\t\tbox_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2]) \n",
    "\t\tbox_predictions[..., 2:] = torch.exp( \n",
    "\t\t\tbox_predictions[..., 2:]) * anchors \n",
    "\t\tscores = torch.sigmoid(predictions[..., 0:1])\n",
    "\t\n",
    "\t# Else we will just calculate scores and best class. \n",
    "\telse: \n",
    "\t\tscores = predictions[..., 0:1]\n",
    "\n",
    "\t# Calculate cell indices \n",
    "\tcell_indices = ( \n",
    "\t\ttorch.arange(s) \n",
    "\t\t.repeat(predictions.shape[0], 3, s, 1) \n",
    "\t\t.unsqueeze(-1) \n",
    "\t\t.to(predictions.device) \n",
    "\t) \n",
    " \n",
    "\n",
    "\t# Calculate x, y, width and height with proper scaling \n",
    "\tx = 1 / s * (box_predictions[..., 0:1] + cell_indices) \n",
    "\ty = 1 / s * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4)) \n",
    "\twidth_height = 1 / s * box_predictions[..., 2:4] \n",
    "\n",
    "\t# Concatinating the values and reshaping them in \n",
    "\t# (BATCH_SIZE, num_anchors * S * S, 6) shape\n",
    "\tconverted_bboxes = torch.cat( \n",
    "\t\t(scores, x, y, width_height), dim=-1\n",
    "\t).reshape(batch_size, num_anchors * s * s, 5) \n",
    "\n",
    "\t# Returning the reshaped and converted bounding box list \n",
    "\treturn converted_bboxes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images with bounding boxes and class labels \n",
    "def plot_image(image, boxes): \n",
    "\n",
    "\t# Reading the image with OpenCV \n",
    "\timg = np.array(image) \n",
    "\t# Getting the height and width of the image \n",
    "\th, w = img.shape \n",
    "\n",
    "\t# Create figure and axes \n",
    "\tfig, ax = plt.subplots(1) \n",
    "\n",
    "\t# Add image to plot \n",
    "\tax.imshow(img, cmap='gray')\n",
    "\n",
    "\t# Plotting the bounding boxes and labels over the image \n",
    "\tfor box in boxes:\n",
    "\t\t# Get the center x and y coordinates \n",
    "\t\tbox = box[1:] \n",
    "\t\t# Get the upper left corner coordinates \n",
    "\t\tupper_left_x = box[0] - box[2] / 2\n",
    "\t\tupper_left_y = box[1] - box[3] / 2\n",
    "\n",
    "\t\t# Create a Rectangle patch with the bounding box \n",
    "\t\trect = patches.Rectangle( \n",
    "\t\t\t(upper_left_x * w, upper_left_y * h), \n",
    "\t\t\tbox[2] * w, \n",
    "\t\t\tbox[3] * h, \n",
    "\t\t\tlinewidth=2, \n",
    "\t\t\tedgecolor='red', \n",
    "\t\t\tfacecolor=\"none\", \n",
    "\t\t) \n",
    "\t\t\n",
    "\t\t# Add the patch to the Axes \n",
    "\t\tax.add_patch(rect)\n",
    "\n",
    "\t# Display the plot \n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4193, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_dims = []\n",
    "with open('../data/AR-MOT/labels.csv', 'r', encoding='utf-8') as label_file:\n",
    "    label_file.readline()\n",
    "    for line in label_file.readlines():\n",
    "        _, x, y, width, height = line.split(',')\n",
    "        box_dims.append([float(width), float(height)])\n",
    "box_dims = np.array(box_dims)\n",
    "box_dims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all_iou(anch_box, boxes):\n",
    "    x = np.minimum(anch_box[0], boxes[:, 0])\n",
    "    y = np.minimum(anch_box[1], boxes[:, 1])\n",
    "    intersection = x * y\n",
    "    anch_box_area = anch_box[0] * anch_box[1]\n",
    "    boxes_area = boxes[:, 0] * boxes[:, 1]\n",
    "    union = anch_box_area + boxes_area - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance(point, points):\n",
    "    return 1 - one_vs_all_iou(point, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(samples, n_clusters, distance_func):\n",
    "    n_samples = samples.shape[0]\n",
    "    distances = np.empty((n_samples, n_clusters))\n",
    "    last_clusters = np.zeros((n_samples))\n",
    "    nearest_clusters = np.full((n_samples), -1)\n",
    "\n",
    "    clusters = samples[np.random.choice(n_samples, n_clusters, replace=False)]\n",
    "\n",
    "    while not (last_clusters == nearest_clusters).all():\n",
    "        last_clusters = nearest_clusters\n",
    "        for i in range(n_clusters):\n",
    "            distances[:, i] = distance_func(clusters[i], samples)\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "        for i in range(n_clusters):\n",
    "            clusters[i] = np.mean(samples[nearest_clusters == i], axis=0)\n",
    "\n",
    "    return clusters, nearest_clusters, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, nearest_clusters, distances = kmeans(box_dims, 9, distance_func=distance)\n",
    "intercluster_mean_distance = np.mean(distances[np.arange(distances.shape[0]), nearest_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = sorted(clusters, key=lambda x: x[0]*x[1])\n",
    "anchors = np.array(anchors).reshape(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 89.75458716, 106.92431193],\n",
       "        [174.64      , 158.47703704],\n",
       "        [289.96757458, 187.5693904 ]],\n",
       "\n",
       "       [[221.38817481, 271.33161954],\n",
       "        [358.9468599 , 263.64251208],\n",
       "        [500.21971253, 321.72689938]],\n",
       "\n",
       "       [[372.8490566 , 432.9509434 ],\n",
       "        [639.91686461, 471.93349169],\n",
       "        [961.578125  , 637.625     ]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class to load the images and labels from the folder \n",
    "class Dataset(torch.utils.data.Dataset): \n",
    "\tdef __init__( \n",
    "\t\tself, image_dir, labels_path, anchors, \n",
    "\t\timage_size=416, grid_sizes=[13, 26, 52], original_image_size=4096\n",
    "\t):\n",
    "\t\tlabels = []\n",
    "\t\twith open(labels_path, 'r', encoding='utf-8') as label_file:\n",
    "\t\t\tlabel_file.readline()\n",
    "\t\t\tlabel = []\n",
    "\t\t\tix_prev = 1\n",
    "\t\t\tfor line in label_file.readlines():\n",
    "\t\t\t\tix, x1, y1, w, h = [float(a) / original_image_size for a in line.split(',')]\n",
    "\t\t\t\tif ix != ix_prev:\n",
    "\t\t\t\t\tlabels.append(label)\n",
    "\t\t\t\t\tlabel = [[x1, y1, w, h]]\n",
    "\t\t\t\t\tix_prev = ix\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlabel.append([x1, y1, w, h])\n",
    "\t\tself.labels = labels #np.array(labels, dtype=np.float32)\n",
    "  \n",
    "\t\t# Image and label directories \n",
    "\t\tself.image_dir = image_dir\n",
    "\t\t# Image size \n",
    "\t\tself.image_size = image_size\n",
    "\t\t# Grid sizes for each scale \n",
    "\t\tself.grid_sizes = grid_sizes \n",
    "\t\t# Anchor boxes \n",
    "\t\tself.anchors = anchors.reshape(-1, 2) / original_image_size\n",
    "\t\t# Number of anchor boxes \n",
    "\t\tself.num_anchors = self.anchors.shape[0] \n",
    "\t\t# Number of anchor boxes per scale \n",
    "\t\tself.num_anchors_per_scale = self.num_anchors // 3\n",
    "\t\t# Ignore IoU threshold \n",
    "\t\tself.ignore_iou_thresh = 0.5\n",
    "\n",
    "\tdef __len__(self): \n",
    "\t\treturn len(self.label_list) \n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = os.path.join(self.image_dir, str(idx+1).zfill(6) + '.jpg') \n",
    "\t\timage = np.array(Image.open(img_path))\n",
    "\t\ttargets = [torch.zeros((self.num_anchors_per_scale, s, s, 5)) \n",
    "\t\t\t\tfor s in self.grid_sizes]\n",
    "\t\tbboxes = self.labels[idx]\n",
    "\t\tfor box in bboxes:\n",
    "\t\t\tiou_anchors = one_vs_all_iou(torch.tensor(box[2:4]), self.anchors)\n",
    "\t\t\tanchor_indices = iou_anchors.argsort(descending=True, dim=0) \n",
    "\t\t\tx, y, width, height = box\n",
    "\t\t\thas_anchor = [False] * 3\n",
    "\t\t\tfor anchor_idx in anchor_indices: \n",
    "\t\t\t\tscale_idx = anchor_idx // self.num_anchors_per_scale \n",
    "\t\t\t\tanchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "\t\t\t\ts = self.grid_sizes[scale_idx]\n",
    "\t\t\t\ti, j = int(s * y), int(s * x) \n",
    "\t\t\t\tanchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "\t\t\t\tif not anchor_taken and not has_anchor[scale_idx]:\n",
    "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "\t\t\t\t\tx_cell, y_cell = s * x - j, s * y - i\n",
    "\t\t\t\t\twidth_cell, height_cell = (width * s, height * s)\n",
    "\t\t\t\t\tbox_coordinates = torch.tensor( \n",
    "\t\t\t\t\t\t\t\t\t\t[x_cell, y_cell, width_cell, \n",
    "\t\t\t\t\t\t\t\t\t\theight_cell] \n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "\t\t\t\t\thas_anchor[scale_idx] = True\n",
    "\t\t\t\telif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "\t\t\t\t\ttargets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\t\treturn image, tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('../data/AR-MOT/images', '../data/AR-MOT/labels.csv', anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = anchors / 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10647\n"
     ]
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "boxes = [] \n",
    "for i in range(3): \n",
    "    anchor = anchors[i] \n",
    "    yy = torch.unsqueeze(y[i], 0)\n",
    "    boxes += convert_cells_to_bboxes( \n",
    "               yy, is_predictions=False, s=y[i].shape[2], anchors=anchor \n",
    "             )[0] \n",
    "print(len(boxes))\n",
    "# Applying non-maximum suppression \n",
    "# boxes = nms(boxes, iou_threshold=1, threshold=0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 34\u001b[0m, in \u001b[0;36mplot_image\u001b[0;34m(image, boxes)\u001b[0m\n\u001b[1;32m     24\u001b[0m \trect \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mRectangle( \n\u001b[1;32m     25\u001b[0m \t\t(upper_left_x \u001b[38;5;241m*\u001b[39m w, upper_left_y \u001b[38;5;241m*\u001b[39m h), \n\u001b[1;32m     26\u001b[0m \t\tbox[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m w, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \t\tfacecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     31\u001b[0m \t) \n\u001b[1;32m     33\u001b[0m \t\u001b[38;5;66;03m# Add the patch to the Axes \u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \t\u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Display the plot \u001b[39;00m\n\u001b[1;32m     37\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Career/DMLab/AR TRACKING/ar-track/src/ByteTrack/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:2413\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_artist_props(p)\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2413\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_clip_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_patch_limits(p)\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "File \u001b[0;32m~/Career/DMLab/AR TRACKING/ar-track/src/ByteTrack/.venv/lib/python3.9/site-packages/matplotlib/artist.py:799\u001b[0m, in \u001b[0;36mArtist.set_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, Rectangle):\n\u001b[0;32m--> 799\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipbox \u001b[38;5;241m=\u001b[39m TransformedBbox(\u001b[43mBbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    800\u001b[0m                                        path\u001b[38;5;241m.\u001b[39mget_transform())\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clippath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Career/DMLab/AR TRACKING/ar-track/src/ByteTrack/.venv/lib/python3.9/site-packages/matplotlib/transforms.py:798\u001b[0m, in \u001b[0;36mBbox.unit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munit\u001b[39m():\n\u001b[1;32m    797\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new unit `Bbox` from (0, 0) to (1, 1).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Career/DMLab/AR TRACKING/ar-track/src/ByteTrack/.venv/lib/python3.9/site-packages/matplotlib/transforms.py:767\u001b[0m, in \u001b[0;36mBbox.__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mpoints : `~numpy.ndarray`\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    A (2, 2) array of the form ``[[x0, y0], [x1, y1]]``.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 767\u001b[0m points \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m points\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBbox points must be of the form \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    770\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[[x0, y0], [x1, y1]]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_image(x, boxes * 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
